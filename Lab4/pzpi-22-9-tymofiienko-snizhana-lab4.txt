Міністерство освіти і науки України Харківський національний університет радіоелектроніки


Кафедра програмної інженерії





Звіт
з лабораторної роботи №4
з дисципліни "Архітектура програмного забезпечення" з теми: "Масштабування бекенда"







Виконала	Перевірив
ст. гр. ПЗПІ-22-9	Дашенков Д. С.
Тимофієнко Сніжана














Харків 2025
1 ІСТОРІЯ ЗМІН
№
Дата
Версія звіту
Опис змін та виправлень
1
02.06.225
0.1
Створено звіт
2 ЗАВДАННЯ
Тема: Масштабування бекенду.
В цій лабораторній роботі необхідно показати як можна масштабувати
бекенд системи для роботи із великим навантаженням. Для цього, можна на вибір: масштабувати сервер горизонтально – багато копій сервера виконують однакові функції для різних користувачів; масштабувати сервер вертикально – різні мікросервіси виконують різні функції і масштабуються окремо одне від одного. На найвищий бал на цю роботу необхідно провести навантажувальне тестування за допомогою Gatling, JMeter, Locust чи іншого подібного інструмента і показати як зі збільшенням кількості серверів зростає кількість запитів на секунду яку витримує система.
3 ОПИС ВИКОНАНОЇ РОБОТИ
3.1 Опис стратегії масштабування системи
У межах масштабування бекенду Pet Health системи було реалізовано горизонтальне масштабування серверної частини у середовищі Docker. Для цього було створено окремі контейнери, що відповідають за обробку запитів.
Архітектура системи включає:
    • Backend контейнери: Go-додаток з chi роутером, побудований з Golang 1.22
    • MongoDB контейнер: Централізована база даних з volume для збереження даних
    • Docker мережа: Забезпечує зв'язок між контейнерами
3.2 Опис технічних рішень, які роблять масштабування можливим
Масштабування в Pet Health системі стало можливим завдяки:
1. Stateless архітектура: Бекенд не зберігає стан між запитами, всі дані зберігаються в MongoDB
2. Docker контейнеризація: Дозволяє легко створювати кілька інстансів додатка
3. Docker мережа: Забезпечує зв'язок між контейнерами за іменами
4. Централізована база даних: MongoDB контейнер з volume забезпечує єдине джерело даних
Ключові файли конфігурації:
    • Dockerfile - мультистейдж збірка Go додатка
    • docker-compose.yml - опис всіх сервісів та їх взаємодії
    • .env - конфігурація змінних оточення
    • simple_test.sh - скрипт для ручного тестування
3.3 Опис навантажувальних тестів
Для тестування системи використано інструмент Locust, який дозволяє моделювати навантаження від великої кількості користувачів.
Конфігурація тестування:
    • Кількість користувачів: 500
    • Швидкість появи: 20 користувачів/секунда
    • Тривалість кожного тесту: 2 хвилини
    • Тестові сценарії: 1, 2 та 3 контейнери
Тестовані ендпоінти:
    • GET / - кореневий endpoint сервера
    • GET /api/pet-and-health/ - API кореневий endpoint
    • GET /health - health check endpoint
    • GET /api/pet-and-health/login/ - login endpoint
Особливості тестування:
    • Використання Docker мережі для зв'язку контейнерів
    • Автоматичне створення та очищення контейнерів
    • Генерація HTML звітів для кожної конфігурації
    • Збір метрик використання ресурсів контейнерів

3.4 Аналіз результатів навантаження
Проведені навантажувальні тести показали позитивний вплив горизонтального масштабування на продуктивність Pet Health системи. Нижче наведено детальний аналіз результатів для кожної конфігурації:
Тестування з 1 контейнером:
При тестуванні системи з одним контейнером було отримано базові показники продуктивності:
    • RPS: 81.96 запитів/секунда
    • Середній час відповіді: 3.57 мс
    • Максимальний час відповіді: 44 мс
    • Помилки: 0 (100% успішних запитів)
Графіки показують стабільне навантаження протягом тесту, проте RPS залишається на відносно низькому рівні ~82 запитів/секунда. Час відповіді залишається стабільним на рівні 3-4 мс.
Рисунок 1 — Результат тестування системи з 1 контейнером
Рисунок 2 - Результат тестування системи з 1 контейнером
Рисунок 3 - Результат тестування системи з 1 контейнером
Тестування з 2 контейнерами:
При масштабуванні до двох контейнерів спостерігається незначне покращення:
    • RPS: 83 запити/секунда
    • Середній час відповіді: 3.32 мс
    • Максимальний час відповіді: 67 мс
    • Помилки: 0 (100% успішних запитів)
Збільшення кількості контейнерів до двох показало мінімальний приріст продуктивності (~1 RPS), що може свідчити про те, що при поточному навантаженні один контейнер справляється з обробкою запитів, а другий контейнер використовується не повною мірою.
Рисунок 4 - Результат тестування системи з 2 контейнерами
Рисунок 5 - Результат тестування системи з 2 контейнерами
Рисунок 6 - Результат тестування системи з 2 контейнерами
Тестування з 3 контейнерами:
Конфігурація з трьома контейнерами продемонструвала найкращі результати:
    • RPS: ~95 запитів/секунда (пікове значення)
    • Середній час відповіді: стабільний на рівні 3-4 мс
    • Максимальний час відповіді: в межах 67 мс
    • Помилки: 0 (100% успішних запитів)
З графіків видно, що система з трьома контейнерами демонструє найвищий і найстабільніший RPS протягом усього тесту, досягаючи пікових значень близько 95 запитів/секунда.
Рисунок 7 - Результат тестування системи з 3 контейнерами
Рисунок 8 - Результат тестування системи з 3 контейнерами
Рисунок 9 - Результат тестування системи з 3 контейнерами

                       Порівняльний аналіз
Результати тестування показують:
1. Зростання продуктивності: RPS збільшився з 82 до 95 запитів/секунда (+15.8%) при переході від 1 до 3 контейнерів
2. Стабільність системи: Всі конфігурації показали 0% помилок та стабільні часи відповіді
3. Ефективність масштабування: Найбільший приріст продуктивності спостерігається при переході до 3 контейнерів
Таблиця 1 – Результати тестування системи під навантаженням
Кількість контейнерів
RPS
Середній час відповіді
Макс. час відповіді
Помилки
1
82
3.57 мс
44 мс
0%
2
83
3.32 мс
67 мс
0%
3
95
3.5 мс
67 мс
0%

Висновки з аналізу
1. Ефективність горизонтального масштабування: Система демонструє здатність до масштабування, хоча приріст продуктивності є помірним
2. Стабільність роботи: Відсутність помилок у всіх конфігураціях свідчить про надійність архітектури
3. Оптимальна конфігурація: Три контейнери показали найкращі результати для даного типу навантаження
Розподіл навантаження між різними ендпоінтами залишався постійним:
    • testRoot: 45.5% запитів
    • testApiRoot: 27.3% запитів
    • testHealthCheck: 18.2% запитів
    • testLoginPage: 9.1% запитів
Цей розподіл відповідає реальним сценаріям використання системи, де кореневі ендпоінти отримують найбільше навантаження.
3.5 Аналіз вузьких місць
Під час проведення навантажувальних тестів було виявлено наступні обмеження:
1. База даних як вузьке місце: MongoDB працює в одному контейнері та може стати обмежувальним фактором при високому навантаженні
2. Мережеві обмеження: У локальному середовищі Docker можуть виникати обмеження пропускної здатності
3. Ресурси хост-машини: При тестуванні на локальній машині ресурси CPU та пам'яті можуть обмежувати продуктивність
4. Відсутність балансувальника навантаження: Тестування проводилось тільки на одному порті (8080), без розподілу навантаження між інстансами
4 ВИСНОВКИ
У ході виконання лабораторної роботи було:
1. Успішно розгорнуто Pet Health систему в Kubernetes з підтримкою горизонтального масштабування
2. Реалізовано балансування навантаження через Kubernetes LoadBalancer Service
3. Проведено навантажувальне тестування з різною кількістю pod-ів
4. Підтверджено ефективність масштабування
5. Ідентифіковано вузькі місця: Основним обмеженням є централізована база даних
Посилання на GitHub: https://github.com/NureTymofiienkoSnizhana/apz-pzpi-22-9-tymofiienko-snizhana/tree/main/Lab4